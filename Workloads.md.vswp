vnote_backup_file_826537664 /home/zyx/kubernetes_doc/Workloads_Template.md
# Workloads
## Pods
### Pod 概述
kubernetes的基础板块、是创建和部署model的最简单元
它包括了一个容器、存储、独特的IP、以及可选的容器管理方法，代表了一个部署的单元：一个应用实例。
Docker是最为k8s pod 普遍的容器运行时，但是pod同时也支持其他的容器运行时。
k8s集群中的pods通常有以下两种作用：单容器、多容器。
和相互独立的容器一样，Pod是一种相对短暂的存在，而不是持久存在的，正如我们在Pod的生命周期中提到的，Pod被安排到结点上，并且保持在这个节点上直到被终止（根据重启的设定）或者被删除，当一个节点死掉之后，上面的所有Pod均会被删除。特殊的Pod永远不会被转移到的其他的节点，作为替代，他们必须被replace.
Pod可以作为垂直应用整合的载体，但是它的主要特点是支持同地协作，同地管理程序，例如：

    内容管理系统，文件和数据加载，本地缓存等等
    日志和检查点备份，压缩，循环，快照等等
    数据交换监控，日志追踪，日志记录和监控适配器，以及事件发布等等
    代理，网桥，适配器
    控制，管理，配置，更新

总体来说，独立的Pod不会去加载多个相同的应用实例
### Pods
![kubernetes_doc](_v_images/20190518142244539_1824819835.svg)
### 生命周期
#### 状态
   pod 的状态定义在 podstatus 中 phase 字段中。phase 是 POD 在其生命周期中简单的宏观描述，以下是Pod的可能值。

   * Peding : 挂起，说明pod已被k8s系统接受，但容器尚未创建。此时容器可能在调度Pod和拉取网络镜像。
   * Running : 运行，pod已经绑定于某一节点，所有容器都已经创建。至少有一个容器正在运行，或正处于启动或重启状态。
   * Succeed ：该Pod中所有容器都被成功终止，并且不会再重启。
   * Failed ：：Pod所有容器都以终止，并且至少有一个容器因为失败而终止。
   * Unknown : 因为某些原因无法取得Pod的状态，通常是因为Pod与Node通信失败。
    上述状态包含在PodStatus对象的PodCondition数组中。
#### 探针
顾名思义，探针的作用就是监控Pod的状态，这种监控是定期的。由于这种监控包含了探测和决策，不妨将其称为诊断，这种诊断是有kubelet调用容器实现的Handler。有以下三种类型的处理程序：
* ExecAction : 在容器能执行制定命令。如果命令的返回码为０，诊断成功。
* TCPSocketAction : 对指定端口的IP进行TCP检查。如果端口打开，则诊断被认为是成功的。
* HTTPGetAction : 根据指定端口和路径上容器的IP地址HTTP响应的状态码是否大于等于200且小于等于400.
每次探测都将获得以下三种结果之一：
* 成功：容器通过了诊断。
* 失败：容器未通过诊断。
* 未知：诊断失败，不采取任何行动。

Kubelet 可以选择是否执行在容器上运行的两种探针执行和做出反应：

   * livenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略 的影响。如果容器不提供存活探针，则默认状态为 Success。
   * readinessProbe：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success。

#### 重启策略
PodSpec 中有一个restartPolicy字段，可能的值为PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为 Always。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10秒，20秒，40秒…）重新启动，并在成功执行十分钟后重置。如 Pod 文档 中所述，一旦绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。


### Init 容器
init容器是一种在应用容器启动之前就运行的专用容器，用于部署实用工具和安装脚本。
#### 特点
Init与普通容器最大的不同在于

* 生命周期和Podx相同。
* 必须在pod下一次启动之前完成。

因此，直到init容器成功为止，k8s会不断重启Pod，当然Pod的重启策略具有更高的优先级。

#### 作用
因为 Init 容器具有与应用容器分离的单独镜像，它们的启动相关代码具有如下优势：

* 它们可以包含并运行实用工具，处于安全考虑，是不建议在应用容器镜像中包含这些实用工具的。
* 它们可以包含使用工具和定制化代码来安装，但是不能出现在应用镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。
* 应用镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。
* 它们使用 Linux Namespace，所以对应用容器具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用容器不能够访问。
* 它们在应用容器启动之前运行完成，然而应用容器并行运行，所以 Init 容器提供了一种简单的方式来阻塞或延迟应用容器的启动，直到满足了一组先决条件。

### Pod Preset
Preset 作为准入控制器，在被启用后，会应用到接受到的pod创建请求当中。并进行如下操作：

1. 检索所有可用 PodPresets 。
2. 检查 PodPreset 的标签选择器与要创建的 pod 的标签是否匹配。
3. 尝试合并 PodPreset 中定义的各种资源，并注入要创建的 pod。
4. 发生错误时抛出事件，该事件记录了 pod 信息合并错误，同时在 不注入 PodPreset 信息的情况下创建 pod。
5. 为改动的 pod spec 添加注解，来表明它被 PodPreset 所修改。 注解形如： podpreset.admission.kubernetes.io/podpreset-<pod-preset name>": "<resource version>"。

#### 启用步骤
为了在集群中使用 Pod Preset，必须确保以下几点：

* 已启用 api 类型 settings.k8s.io/v1alpha1/podpreset。 这可以通过在 API 服务器的 --runtime-config 配置项中包含 settings.k8s.io/v1alpha1=true 来实现。
* 已启用准入控制器 PodPreset。 启用的一种方式是在 API 服务器的 --admission-control 配置项中包含 PodPreset 。
* 已经通过在相应的名字空间中创建 PodPreset 对象，定义了 Pod preset。

### Kubernetes Pod Disruption Budgets
PDB是实现高可用集群的重要组分。
我们在部署中时常会遇到以下问题：

* 物理主机的硬件错误
* 集群管理误删了VM
* 云服务提供者或虚拟机管理错误使得VM消失
* 内核错误
* 网络不稳导致的容器宕机
这些问题对于用户来说是普遍的，而且也并非是k8s所特有的。我们这些情况归类未自发中断。
包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者操作包括：

* 删除管理该 pod 的 Deployment 或其他控制器
* 更新了 Deployment 的 pod 模板导致 pod 重启
* 直接删除 pod（意外删除）

#### 中断处理
为了避免非自发中断，有以下几种方法

* 确保pod请求的资源充足。
* 为保证高可用性，为所有应用创建副本。
* 同样是为了保证可用性，在不同的节点或区域部署应用的副本。

Kubernetes 提供的功能可以满足在频繁地自动中断的同时运行高可用的应用程序。我们称之为“中断预算”。

#### 中断处理的工作原理
应用程序所有者可以为每个应用程序创建一个 PodDisruptionBudget 对象（PDB）。 PDB 将限制在同一时间自愿中断的复制应用程序中宕机的 Pod 的数量。例如，基于定额的应用程序希望确保运行的副本数量永远不会低于仲裁所需的数量。Web 前端可能希望确保提供负载的副本的数量永远不会低于总数的某个百分比。
集群管理器和托管提供商应使用遵循 Pod Disruption Budgets 的工具，方法是调用Eviction API而不是直接删除 Pod。例如 kubectl drain 命令和 Kubernetes-on-GCE 集群升级脚本（cluster/gce/upgrade.sh）。
当集群管理员想要排空节点时，可以使用 kubectl drain 命令。该命令会试图驱逐机器上的所有 pod。驱逐请求可能会暂时被拒绝，并且该工具会定期重试所有失败的请求，直到所有的 pod 都被终止，或者直到达到配置的超时时间。
PDB 指定应用程序可以容忍的副本的数量，相对于应该有多少副本。例如，具有 spec.replicas：5 的 Deployment 在任何给定的时间都应该有 5 个 Pod。如果其 PDB 允许在某一时刻有 4 个副本，那么驱逐 API 将只允许仅有一个而不是两个 Pod 自愿中断。
使用标签选择器来指定应用程序的一组 pod，这与应用程序的控制器（Deployment、StatefulSet 等）使用的相同。
Pod 控制器的 .spec.replicas 计算“预期的” pod 数量。使用对象的 .metadata.ownerReferences 值从控制器获取。
PDB 不能阻止非自愿中断的发生，但是它们确实会影响预算。
由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入中断预算，但控制器（如 Deployment 和 StatefulSet）在进行滚动升级时不受 PDB 的限制——在应用程序更新期间的故障处理是在控制器的规格（spec）中配置（了解更新 Deployment）。
使用驱逐 API 驱逐 pod 时，pod 会被优雅地终止（请参阅 PodSpec 中的 terminationGracePeriodSeconds）。

## Controllers
### ReplicaSet
ReplicaSet的目的在于使得一系列稳定的Pod在一段给定的时间内稳定运行。例如，保证一定数量的Pod的可用性。
#### 工作原理
ReplicaSet的工作是确保pod的数量始终与其标签选择器匹配。如果不匹配，则ReplicaSet将采取适当的操作来协调pod的数量。
![ReplicaSet](_v_images/20190518205702548_1407973124.png)
#### 工作场景
ReplicaSet能确保运行指定数量的pod。然而，Deployment 是一个更高层次的概念，它能管理ReplicaSets，并提供对pod的更新等功能。因此，我们建议你使用Deployment来管理ReplicaSets，除非你需要自定义更新编排。
这意味着你可能永远不需要操作ReplicaSet对象，而是使用Deployment替代管理 。
大多数kubectl 支持Replication Controller 命令的也支持ReplicaSets。rolling-update命令除外，如果要使用rolling-update，请使用Deployments来实现。
虽然ReplicaSets可以独立使用，但它主要被 Deployments用作pod 机制的创建、删除和更新。当使用Deployment时，你不必担心创建pod的ReplicaSets，因为可以通过Deployment实现管理ReplicaSets。
#### 用例

``` frantend.ymal
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
```
将此配置保存到（frontend.yaml）并提交到Kubernetes集群时，将创建定义的ReplicaSet及其管理的pod。
```
$ kubectl create -f frontend.yaml
replicaset "frontend" created
$ kubectl describe rs/frontend
Name:          frontend
Namespace:     default
Image(s):      gcr.io/google_samples/gb-frontend:v3
Selector:      tier=frontend,tier in (frontend)
Labels:        app=guestbook,tier=frontend
Replicas:      3 current / 3 desired
Pods Status:   3 Running / 0 Waiting / 0 Succeeded / 0 Failed
No volumes.
Events:
  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----                -------------    --------    ------            -------
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-qhloh
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-9si5l
$ kubectl get pods
NAME             READY     STATUS    RESTARTS   AGE
frontend-9si5l   1/1       Running   0          1m
frontend-dnjpy   1/1       Running   0          1m
frontend-qhloh   1/1       Running   0          1m
```
### Deployment
Deployment为Pod和Replica Set（升级版的 Replication Controller）提供声明式更新。

你只需要在 Deployment 中描述您想要的目标状态是什么，Deployment controller 就会帮您将 Pod 和ReplicaSet 的实际状态改变到您的目标状态。您可以定义一个全新的 Deployment 来创建 ReplicaSet 或者删除已有的 Deployment 并创建一个新的来替换。

注意：您不该手动管理由 Deployment 创建的 Replica Set，否则您就篡越了 Deployment controller 的职责！下文罗列了 Deployment 对象中已经覆盖了所有的用例。如果未有覆盖您所有需要的用例，请直接在 Kubernetes 的代码库中提 issue。

典型的用例如下：

* 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。
* 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。
* 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。
* 扩容Deployment以满足更高的负载。
* 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。
* 根据Deployment 的状态判断上线是否hang住了。
* 清除旧的不必要的 ReplicaSet。

#### 创建 Deployment

下面是一个 Deployment 示例，它创建了一个 ReplicaSet 来启动3个 nginx pod。

下载示例文件并执行命令：
```
$ kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record
deployment "nginx-deployment" created
```
将kubectl的 --record 的 flag 设置为 true可以在 annotation 中记录当前命令创建或者升级了该资源。这在未来会很有用，例如，查看在每个 Deployment revision 中执行了哪些命令。

然后立即执行 get 将获得如下结果：
```
$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         0         0            0           1s
```
输出结果表明我们希望的repalica数是3（根据deployment中的.spec.replicas配置）当前replica数（ .status.replicas）是0, 最新的replica数（.status.updatedReplicas）是0，可用的replica数（.status.availableReplicas）是0。
过几秒后再执行get命令，将获得如下输出：

```
$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           18s

```
我们可以看到Deployment已经创建了3个 replica，所有的 replica 都已经是最新的了（包含最新的pod template），可用的（根据Deployment中的.spec.minReadySeconds声明，处于已就绪状态的pod的最少个数）。执行kubectl get rs和kubectl get pods会显示Replica Set（RS）和Pod已创建。
```
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-2035384211   3         3         0       18s
```

您可能会注意到 ReplicaSet 的名字总是<Deployment的名字>-<pod template的hash值>。
```
$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-2035384211-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
```
刚创建的Replica Set将保证总是有3个 nginx 的 pod 存在。
注意： 您必须在 Deployment 中的 selector 指定正确的 pod template label（在该示例中是 app = nginx），不要跟其他的 controller 的 selector 中指定的 pod template label 搞混了（包括 Deployment、Replica Set、Replication Controller 等）。Kubernetes 本身并不会阻止您任意指定 pod template label ，但是如果您真的这么做了，这些 controller 之间会相互打架，并可能导致不正确的行为。
#### Pod-template-hash label
注意：这个 label 不是用户指定的！

注意上面示例输出中的 pod label 里的 pod-template-hash label。当 Deployment 创建或者接管 ReplicaSet 时，Deployment controller 会自动为 Pod 添加 pod-template-hash label。这样做的目的是防止 Deployment 的子ReplicaSet 的 pod 名字重复。通过将 ReplicaSet 的 PodTemplate 进行哈希散列，使用生成的哈希值作为 label 的值，并添加到 ReplicaSet selector 里、 pod template label 和 ReplicaSet 管理中的 Pod 上。
### DaemonSet
#### 什么是 DaemonSeto？
DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。当有节点加入集群时，也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。

使用 DaemonSet 的一些典型用法：

* 运行集群存储 daemon，例如在每个节点上运行 glusterd、ceph。
* 在每个节点上运行日志收集 daemon，例如fluentd、logstash。
* 在每个节点上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。

一个简单的用法是在所有的节点上都启动一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法是单独对每种 daemon 类型使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有不同的内存、CPU要求。
#### 编写 DaemonSet 规约
##### 必需字段
和其它所有 Kubernetes 配置一样，DaemonSet 需要 apiVersion、kind 和 metadata 字段。 有关配置文件的基本信息，详见文档 deploying applications、配置容器 和 资源管理 。
DaemonSet 也需要一个 .spec 配置段。
##### Pod 模板
.spec 唯一必需的字段是 .spec.template。
.spec.template 是一个 Pod 模板。 它与 Pod 具有相同的 schema，除了它是嵌套的，而且不具有 apiVersion 或 kind 字段。
除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 Pod Selector）。
在 DaemonSet 中的 Pod 模板必须具有一个值为 Always 的 RestartPolicy，或者未指定它的值，默认是 Always。
##### Pod Selector
.spec.selector 字段表示 Pod Selector，它与 Job 或其它资源的 .spec.selector 的作用是相同的。

spec.selector 表示一个对象，它由如下两个字段组成：

* matchLabels - 与 ReplicationController 的 .spec.selector 的作用相同。
* matchExpressions - 允许构建更加复杂的 Selector，可以通过指定 key、value 列表，以及与 key 和 value 列表相关的操作符。

当上述两个字段都指定时，结果表示的是 AND 关系。

如果指定了 .spec.selector，必须与 .spec.template.metadata.labels 相匹配。如果没有指定，它们默认是等价的。如果与它们配置的不匹配，则会被 API 拒绝。

如果 Pod 的 label 与 selector 匹配，或者直接基于其它的 DaemonSet、或者 Controller（例如 ReplicationController），也不可以创建任何 Pod。 否则 DaemonSet Controller 将认为那些 Pod 是它创建的。Kubernetes 不会阻止这样做。一个场景是，可能希望在一个具有不同值的、用来测试用的节点上手动创建 Pod。
#### 如何调度 Daemon Pod
正常情况下，Pod 运行在哪个机器上是由 Kubernetes 调度器来选择的。然而，由 Daemon Controller 创建的 Pod 已经确定了在哪个机器上（Pod 创建时指定了 .spec.nodeName），因此：

* DaemonSet Controller 并不关心一个节点的 unschedulable 字段。
* DaemonSet Controller 可以创建 Pod，即使调度器还没有启动，这对集群启动是非常有帮助的。

Daemon Pod 关心 Taint 和 Toleration，它们会为没有指定 tolerationSeconds 的 node.kubernetes.io/not-ready 和 node.alpha.kubernetes.io/unreachable 的 Taint，创建具有 NoExecute 的 Toleration。这确保了当 alpha 特性的 TaintBasedEvictions 被启用时，发生节点故障，比如网络分区，这时它们将不会被清除掉（当 TaintBasedEvictions 特性没有启用，在这些场景下也不会被清除，但会因为 NodeController 的硬编码行为而被清除，而不会因为 Toleration 导致被清除）。
#### 与 Daemon Pod 通信
与 DaemonSet 中的 Pod 进行通信，几种可能的模式如下：

* Push：配置 DaemonSet 中的 Pod 向其它 Service 发送更新，例如统计数据库。它们没有客户端。
* NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod。客户端能通过某种方法知道节点 IP 列表，并且基于此也可以知道端口。
* DNS：创建具有相同 Pod Selector 的 Headless Service，然后通过使用 endpoints 资源或从 DNS 检索到多个 A 记录来发现 DaemonSet。
* Service：创建具有相同 Pod Selector 的 Service，并使用该 Service 随机访问到某个节点上的 daemon（没有办法访问到特定节点）。
#### 更新 DaemonSet
如果修改了节点标签（Label），DaemonSet 将立刻向新匹配上的节点添加 Pod，同时删除新近不能够匹配的节点上的 Pod。
我们可以修改 DaemonSet 创建的 Pod。然而，不允许对 Pod 的所有字段进行更新。当下次节点（即使具有相同的名称）被创建时，DaemonSet Controller 还会使用最初的模板。
可以删除一个 DaemonSet。如果使用 kubectl 并指定 --cascade=false 选项，则 Pod 将被保留在节点上。然后可以创建具有不同模板的新 DaemonSet。具有不同模板的新 DaemonSet 将能够通过标签匹配并识别所有已经存在的 Pod。它不会修改或删除它们，即使是错误匹配了 Pod 模板。通过删除 Pod 或者删除节点，可以强制创建新的 Pod。
在 Kubernetes 1.6 或以后版本，可以在 DaemonSet 上 执行滚动升级。
未来的 Kubernetes 版本将支持节点的可控更新。